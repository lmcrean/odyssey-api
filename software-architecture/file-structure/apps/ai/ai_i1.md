# Apps/AI - Odyssey Creator Platform (MVP - Iteration 1)

> **Simple AI Chat Service** - MVP implementation with basic Gemini integration

## Architecture Overview

The AI app provides basic chat functionality using Google's Gemini API. This is **Iteration 1 (MVP)** focused on core chat features only.

## Complete AI Message Flow

```typescript
ğŸ“± Frontend (apps/web)
    â†“ POST /api/chat/send-message { message, userId }
ğŸ”§ API (apps/api) 
    â†“ Saves user message to DB
    â†“ HTTP POST to odyssey-ai-lmcreans-projects.vercel.app/api/chat/send-message
ğŸ¤– AI Service (apps/ai)
    â†“ Processes with Gemini API
    â†‘ Returns AI response { content, metadata }
ğŸ”§ API (apps/api)
    â†‘ Saves AI message to DB  
    â†‘ Returns complete conversation to Frontend
ğŸ“± Frontend receives: { userMessage, aiMessage, conversationId }
```

## Deployment Structure

```typescript
// Separate Vercel deployment
apps/ai/ â†’ odyssey-ai-lmcreans-projects.vercel.app

// Service communication
apps/web/ â†’ apps/api/ â†’ apps/ai/
              â†“         â†‘
         Database   AI Processing
         (Stores)   (Stateless)
```

## MVP Directory Structure

```typescript
apps/ai/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ apps/
â”‚   â”‚   â””â”€â”€ chat/                    # Core conversational AI functionality
â”‚   â”‚       â”œâ”€â”€ routes/
â”‚   â”‚       â”‚   â”œâ”€â”€ send-initial-message/    # New conversation starter
â”‚   â”‚       â”‚   â”‚   â”œâ”€â”€ Controller.ts        # Context-aware initial responses
â”‚   â”‚       â”‚   â”‚   â””â”€â”€ Route.ts
â”‚   â”‚       â”‚   â”œâ”€â”€ send-follow-up-message/  # Ongoing conversation
â”‚   â”‚       â”‚   â”‚   â”œâ”€â”€ Controller.ts        # Context-aware follow-ups
â”‚   â”‚       â”‚   â”‚   â””â”€â”€ Route.ts
â”‚   â”‚       â”‚   â”œâ”€â”€ get-conversation-context/ # Context retrieval for API
â”‚   â”‚       â”‚   â”‚   â”œâ”€â”€ Controller.ts
â”‚   â”‚       â”‚   â”‚   â””â”€â”€ Route.ts
â”‚   â”‚       â”‚   â””â”€â”€ validate-conversation/    # Conversation state validation
â”‚   â”‚       â”‚       â”œâ”€â”€ Controller.ts
â”‚   â”‚       â”‚       â””â”€â”€ Route.ts
â”‚   â”‚       â”œâ”€â”€ services/
â”‚   â”‚       â”‚   â”œâ”€â”€ GeminiService.ts          # Core Gemini API integration
â”‚   â”‚       â”‚   â”œâ”€â”€ ConversationProcessor.ts  # Conversation state management
â”‚   â”‚       â”‚   â”œâ”€â”€ PromptBuilder.ts          # Dynamic prompt generation
â”‚   â”‚       â”‚   â”œâ”€â”€ ResponseProcessor.ts      # AI response processing
â”‚   â”‚       â”‚   â”œâ”€â”€ ContextManager.ts         # Conversation context handling
â”‚   â”‚       â”‚   â”œâ”€â”€ AssessmentHandler.ts      # Assessment integration logic
â”‚   â”‚       â”‚   â””â”€â”€ FallbackManager.ts        # Mock/fallback responses
â”‚   â”‚       â”œâ”€â”€ middleware/
â”‚   â”‚       â”‚   â”œâ”€â”€ ConversationValidator.ts  # Validate conversation state
â”‚   â”‚       â”‚   â”œâ”€â”€ ContextInjector.ts        # Inject conversation context
â”‚   â”‚       â”‚   â””â”€â”€ ResponseFormatter.ts      # Standardize response format
â”‚   â”‚       â””â”€â”€ types/
â”‚   â”‚           â”œâ”€â”€ conversation.ts           # Conversation state types
â”‚   â”‚           â”œâ”€â”€ context.ts               # Context management types
â”‚   â”‚           â”œâ”€â”€ assessment.ts            # Assessment integration types
â”‚   â”‚           â”œâ”€â”€ prompts.ts               # Prompt building types
â”‚   â”‚           â”œâ”€â”€ gemini.ts                # Gemini-specific types
â”‚   â”‚           â””â”€â”€ responses.ts             # Response format types
â”‚   â”‚
â”‚   â”œâ”€â”€ shared/
â”‚   â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â”‚   â”œâ”€â”€ gemini.config.ts             # Gemini configuration
â”‚   â”‚   â”‚   â”œâ”€â”€ prompts.config.ts            # System prompts
â”‚   â”‚   â”‚   â””â”€â”€ safety.config.ts             # Safety settings
â”‚   â”‚   â”œâ”€â”€ middleware/
â”‚   â”‚   â”‚   â”œâ”€â”€ auth.ts                      # Internal API token validation
â”‚   â”‚   â”‚   â”œâ”€â”€ rateLimit.ts                 # Basic rate limiting
â”‚   â”‚   â”‚   â””â”€â”€ errorHandler.ts              # Centralized error handling
â”‚   â”‚   â”œâ”€â”€ utilities/
â”‚   â”‚   â”‚   â”œâ”€â”€ messageUtils.ts              # Message formatting utilities
â”‚   â”‚   â”‚   â”œâ”€â”€ contextUtils.ts              # Context manipulation utilities
â”‚   â”‚   â”‚   â””â”€â”€ validationUtils.ts           # Input validation utilities
â”‚   â”‚   â””â”€â”€ types/
â”‚   â”‚       â”œâ”€â”€ api.ts                       # API request/response types
â”‚   â”‚       â”œâ”€â”€ internal.ts                  # Internal service types
â”‚   â”‚       â””â”€â”€ common.ts                    # Shared common types
â”‚   â”‚
â”‚   â””â”€â”€ server.ts
â”œâ”€â”€ vercel.json
â”œâ”€â”€ package.json
â””â”€â”€ tsconfig.json
```

## Migration from Archive (TypeScript Conversion)

```typescript
// Current location â†’ New location (JS â†’ TS conversion required)
.archive/backend-js-reference/routes/chat/ â†’ apps/ai/src/apps/chat/routes/

// Files to migrate and convert to TypeScript:
- send-initial-message/controller.js â†’ apps/ai/src/apps/chat/routes/send-message/Controller.ts
- send-follow-up-message/controller.js â†’ apps/ai/src/apps/chat/services/MessageProcessor.ts
- [gemini integration] â†’ apps/ai/src/apps/chat/services/GeminiService.ts

// Archive has working Gemini integration - convert to TypeScript:
- GoogleGenerativeAI setup âœ…
- System prompts âœ…  
- Conversation history handling âœ…
- Error handling with fallbacks âœ…
```

## Database Strategy (Important Clarification)

```typescript
// DATABASE OWNERSHIP: API app stores everything, AI app is stateless

ğŸ“¦ apps/api (Database Owner)
â”œâ”€â”€ Stores user messages
â”œâ”€â”€ Stores AI responses  
â”œâ”€â”€ Manages conversation history
â””â”€â”€ Handles user authentication

ğŸ¤– apps/ai (Stateless Processor)
â”œâ”€â”€ Receives: { userId, message, conversationHistory }
â”œâ”€â”€ Processes with Gemini
â”œâ”€â”€ Returns: { content, metadata }
â””â”€â”€ NO database operations
```

## API Integration Pattern

```typescript
// apps/api/src/apps/chat/services/AIService.ts

export class AIService {
  private aiBaseUrl = 'https://odyssey-ai-lmcreans-projects.vercel.app';
  
  async processMessage(userId: string, message: string, conversationHistory: Message[]) {
    const response = await fetch(`${this.aiBaseUrl}/api/chat/send-message`, {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${process.env.INTERNAL_AI_TOKEN}`,
        'Content-Type': 'application/json'
      },
      body: JSON.stringify({ 
        userId, 
        message,
        conversationHistory: conversationHistory.slice(-10) // Last 10 messages for context
      })
    });
    
    if (!response.ok) {
      throw new Error(`AI Service error: ${response.status}`);
    }
    
    return response.json() as Promise<{
      content: string;
      metadata: {
        model: string;
        tokens: number;
        responseTime: number;
      }
    }>;
  }
}

// Usage in apps/api chat controller:
export class ChatController {
  async sendMessage(req: Request, res: Response) {
    const { message } = req.body;
    const userId = req.user.id;
    
    try {
      // 1. Save user message to database
      const userMessage = await saveUserMessage(userId, message);
      
      // 2. Get conversation history
      const history = await getConversationHistory(userMessage.conversationId);
      
      // 3. Get AI response
      const aiResponse = await this.aiService.processMessage(userId, message, history);
      
      // 4. Save AI message to database
      const aiMessage = await saveAIMessage(userMessage.conversationId, aiResponse.content);
      
      // 5. Return complete conversation update
      res.json({
        success: true,
        userMessage,
        aiMessage,
        conversationId: userMessage.conversationId
      });
      
    } catch (error) {
      // Fallback handling
      res.status(500).json({ error: 'AI service unavailable' });
    }
  }
}
```

## AI Service Response Format

```typescript
// apps/ai/src/apps/chat/types/chat.ts

export interface AIProcessRequest {
  userId: string;
  message: string;
  conversationHistory?: Array<{
    role: 'user' | 'assistant';
    content: string;
    createdAt: string;
  }>;
}

export interface AIProcessResponse {
  success: boolean;
  content: string;
  metadata: {
    model: 'gemini-2.0-flash' | 'gemini-pro';
    tokens: number;
    responseTime: number;
    conversationLength: number;
    contextUsed: boolean;
  };
  error?: string;
}
```

## Environment Variables

```bash
# AI App (.env) - MVP
GEMINI_API_KEY=...              # Primary AI provider
INTERNAL_AI_TOKEN=...           # Secure APIâ†”AI communication (NO DATABASE ACCESS NEEDED)
NODE_ENV=...                    # Environment
```

## Vercel Configuration

```json
// apps/ai/vercel.json
{
  "version": 2,
  "builds": [
    {
      "src": "src/server.ts",
      "use": "@vercel/node"
    }
  ],
  "routes": [
    {
      "src": "/api/(.*)",
      "dest": "/src/server.ts"
    }
  ],
  "env": {
    "NODE_ENV": "production"
  }
}
```

## Iteration 1 (MVP) Features

### âœ… Included in MVP
- Basic Gemini chat integration (convert from existing JS archive)
- Process message with conversation context
- Return AI responses with metadata
- Internal service authentication
- Basic error handling with fallbacks
- TypeScript implementation

### ğŸ”„ MVP but Minimal Implementation
- Conversation context (last 10 messages only)
- Basic rate limiting per request

### âŒ Not Included in MVP
- Advanced memory system
- AI personality evolution
- AI content generation for creators
- Multiple AI providers (OpenAI, Claude)
- AI agents and automation
- Multimodal AI (video, voice, images)
- Cost optimization and monitoring
- Advanced caching strategies
- Persistent conversation storage (handled by API app)
- Usage analytics

## Testing Strategy

```typescript
// MVP testing focus
npm run test "apps/ai"           # All AI tests
npm run test "GeminiService"     # AI provider integration
npm run test "MessageProcessor"  # Message processing logic

// Integration testing with API app
npm run test:integration         # Test API â†” AI communication
npm run test:prod                # Production environment testing
```

## Implementation Timeline

### **Week 1: Setup & Migration**
- âœ… Set up apps/ai structure
- âœ… Convert archive JS files to TypeScript
- âœ… Implement GeminiService (from archive)
- âœ… Create send-message endpoint

### **Week 2: Integration & Testing**
- âœ… Integrate with apps/api via HTTP
- âœ… Test internal authentication
- âœ… Unit tests for core functionality
- âœ… Deploy to Vercel

### **Week 3: Production Testing**
- âœ… Integration tests with main API
- âœ… Production environment testing
- âœ… Performance optimization

## Security & Privacy (MVP)

### **Basic Security**
- Bearer token authentication between APIâ†”AI
- Input validation and sanitization
- No direct database access from AI service
- Internal service communication only

### **Data Protection**
- No persistent storage in AI service
- Conversation data handled by API app only
- Basic privacy controls
- User data anonymization in logs

## Benefits of This Architecture

### ğŸš€ **Service Separation**
- AI service is stateless and scalable
- Database operations centralized in API
- Clear responsibility boundaries
- Easy to maintain and debug

### ğŸ’° **Cost Effective**
- Simple Gemini integration (existing archive code)
- No complex infrastructure
- Minimal third-party services
- Predictable costs

### ğŸ”§ **Easy Integration**
- HTTP-based communication
- Standard REST API patterns
- Existing Gemini code can be reused
- TypeScript for better maintainability

### ğŸ“ˆ **Foundation for Growth**
- Clean architecture ready for enhancements
- Service can be scaled independently
- API structure extensible
- Easy to add more AI providers later

## Key Implementation Notes

1. **Database Strategy**: Only apps/api touches the database - apps/ai is purely for processing
2. **Archive Migration**: Convert existing working JS Gemini code to TypeScript
3. **Error Handling**: AI service failures should not break the chat experience
4. **Authentication**: Internal token between services, no user auth in AI service
5. **Context**: Pass conversation history to AI service for better responses

## Next Steps to Iteration 2

After MVP validation, expand to include:
1. Advanced memory system
2. AI content generation features
3. Multiple AI provider support
4. Cost optimization
5. Enhanced user experience 

## What the Archive Code Reveals We Need

### **1. Conversation Type Handling**
```typescript
// The archive has separate logic for:
- Initial messages (with assessment context)
- Follow-up messages (with conversation history)
- Different system prompts per conversation type
- Assessment integration logic
```

### **2. Complex Prompt Management**
```typescript
// Multiple prompt types needed:
- SYSTEM_PROMPT for base personality
- Assessment-aware prompts
- Context-enhanced prompts for follow-ups
- Safety and content filtering
```

### **3. Conversation Context Processing**
```typescript
// Archive shows complex context handling:
- Converting conversation history to Gemini format
- Managing conversation state
- Assessment data integration
- History length management (performance)
```

### **4. Sophisticated Response Processing**
```typescript
// Response handling includes:
- Mock response generation (for development)
- Fallback responses (when API fails)
- Response formatting and metadata
- Error recovery strategies
```

### **5. State Management**
```typescript
// Conversation state complexity:
- Assessment linking
- Conversation history tracking
- User context preservation
- Performance optimization
```

## API Endpoints & Conversation Flow

### **Required Endpoints (Based on Archive Analysis)**

```typescript
// 1. Initial Message Processing
POST /api/chat/send-initial-message
{
  userId: string;
  message: string;
  conversationId: string;
  assessmentId?: string;        # Links to user assessment
  assessmentData?: object;      # Assessment context
}

// 2. Follow-up Message Processing  
POST /api/chat/send-follow-up-message
{
  userId: string;
  message: string;
  conversationId: string;
  conversationHistory: Message[];  # Last 10-20 messages for context
}

// 3. Conversation Context Retrieval
GET /api/chat/conversation-context/:conversationId
# Returns: conversation state, assessment links, context summary

// 4. Health/Status Check
GET /api/chat/health
# Returns: service status, Gemini API availability
```

### **Complex Conversation Processing**

```typescript
// apps/ai/src/apps/chat/services/ConversationProcessor.ts

export class ConversationProcessor {
  
  // Handle initial message with assessment context
  async processInitialMessage(request: InitialMessageRequest): Promise<AIResponse> {
    // 1. Assessment context integration
    const assessmentContext = await this.assessmentHandler.getContext(request.assessmentId);
    
    // 2. Build enhanced prompt with assessment data
    const systemPrompt = this.promptBuilder.buildInitialPrompt(assessmentContext);
    
    // 3. Process with Gemini
    const geminiResponse = await this.geminiService.sendInitialMessage(
      systemPrompt,
      request.message,
      assessmentContext
    );
    
    // 4. Process and format response
    return this.responseProcessor.formatInitialResponse(geminiResponse, assessmentContext);
  }
  
  // Handle follow-up with conversation history
  async processFollowUpMessage(request: FollowUpMessageRequest): Promise<AIResponse> {
    // 1. Process conversation history
    const contextSummary = this.contextManager.buildContextSummary(request.conversationHistory);
    
    // 2. Convert to Gemini format
    const geminiHistory = this.contextManager.formatForGemini(request.conversationHistory);
    
    // 3. Build context-aware prompt
    const enhancedPrompt = this.promptBuilder.buildFollowUpPrompt(contextSummary);
    
    // 4. Process with conversation context
    const geminiResponse = await this.geminiService.sendFollowUpMessage(
      geminiHistory,
      request.message,
      enhancedPrompt
    );
    
    // 5. Process and format response
    return this.responseProcessor.formatFollowUpResponse(geminiResponse, contextSummary);
  }
}
```

### **Assessment Integration Complexity**

```typescript
// apps/ai/src/apps/chat/services/AssessmentHandler.ts

export class AssessmentHandler {
  
  // Extract assessment context for AI
  async getAssessmentContext(assessmentId: string): Promise<AssessmentContext> {
    // Complex logic from archive:
    // - Assessment pattern recognition
    // - Symptom context extraction  
    // - Personalization data
    // - Medical context awareness
    
    return {
      pattern: 'irregular_periods',
      symptoms: ['heavy_flow', 'pain'],
      personalizedGuidance: true,
      medicalDisclaimerRequired: true
    };
  }
  
  // Build assessment-aware prompts
  buildAssessmentPrompt(assessmentData: AssessmentContext): string {
    // From archive: complex prompt building with:
    // - User's specific assessment results
    // - Personalized system prompts
    // - Medical disclaimer integration
    // - Context-specific guidance
    
    return `You are Dottie, responding to a user with ${assessmentData.pattern}...`;
  }
}
```

### **Fallback & Error Management**

```typescript
// apps/ai/src/apps/chat/services/FallbackManager.ts

export class FallbackManager {
  
  // Generate contextual mock responses (from archive)
  generateInitialFallback(message: string, assessmentId?: string): string {
    // Complex fallback logic based on:
    // - Message content analysis
    // - Assessment context
    // - Conversation type
    
    if (message.includes('irregular')) {
      return "Irregular periods can have many causes...";
    }
    // ... more complex logic
  }
  
  // Generate follow-up fallbacks
  generateFollowUpFallback(message: string, conversationHistory: Message[]): string {
    // Context-aware fallbacks based on:
    // - Previous conversation
    // - Current message intent
    // - Conversation flow
  }
}
``` 